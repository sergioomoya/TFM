(Performance_Metrics)=
# Introducción

Este capítulo analiza cómo evaluar el rendimiento de un sistema de detección de fraude. Intuitivamente, la tarea parece simple. Un sistema de detección de fraude ideal debería maximizar el número de clasificaciones correctas y detectar todas las transacciones fraudulentas. Por lo tanto, es tentador pensar que simplemente minimizar la proporción de transacciones mal clasificadas (una métrica conocida como *error medio de clasificación errónea*) es la métrica a optimizar. 

Como mostraremos en breve, el error medio de clasificación errónea es una métrica de rendimiento deficiente, debido a la naturaleza sensible al costo y desequilibrada de un problema de detección de fraude. Una forma sencilla de ilustrar esto es observar que, para un conjunto de datos de transacciones con un 0,1% de transacciones fraudulentas, un modelo de referencia ficticio que clasifica todas las transacciones como genuinas proporciona una precisión muy alta de 0,99. Esto es ampliamente reconocido en la literatura de detección de fraude, y por lo tanto se utilizan comúnmente otras métricas de rendimiento {cite}`tharwat2020classification,dal2017credit,elkan2001foundations `. Las más comunes son el *recall* (recuperación), la *especificidad*, la *precisión*, el *F1 score*, el *AUC ROC* y la *Average Precision* (Precisión Promedio).

En las siguientes secciones, detallaremos estas métricas y discutiremos sus pros y sus contras. Mostraremos que, a pesar de su papel central en la evaluación de un sistema de detección de fraude, en realidad no hay consenso sobre qué métrica debe utilizarse. 

Las métricas de recall, especificidad, precisión y F1 score, también conocidas como métricas *basadas en umbrales*, tienen limitaciones bien conocidas debido a su dependencia de un umbral de decisión que es difícil de determinar en la práctica y depende en gran medida de las restricciones específicas del negocio. A menudo se complementan con el AUC ROC y, más recientemente, con las métricas de Precisión Promedio (AP). Las métricas AUC ROC y AP tienen como objetivo evaluar, con un solo número, el rendimiento para todos los umbrales de decisión posibles, y se denominan métricas *libres de umbral*. El AUC ROC es actualmente la métrica de facto para evaluar las precisiones de detección de fraude {cite}`chawla2009data,dal2015adaptive`. Sin embargo, investigaciones recientes han demostrado que esta métrica también es engañosa para evaluar problemas altamente desequilibrados como la detección de fraude {cite}`muschelli2019roc`, y recomendaron usar la curva de Precisión-Recuperación y la métrica AP en su lugar {cite}`saito2015precision,boyd2013area`. 

El capítulo está estructurado de la siguiente manera. La [Sección 4.2](Threshold_Based_Metrics) presenta primero la detección de fraude como un problema de clasificación y detalla las principales métricas basadas en umbrales. A través de un ejemplo simple, mostramos que el error medio de clasificación errónea es un mal indicador de rendimiento y motivamos el uso de métricas alternativas como recall, especificidad, precisión y F1-score. La [Sección 4.3](Threshold_Free_Metrics) analiza el uso de medidas libres de umbral como AUC ROC y AP, y muestra sus beneficios y limitaciones. La [Sección 4.4](Precision_Top_K_Metrics) finalmente aborda el problema de detección de fraude desde una perspectiva más operativa y motiva el uso de la métrica *Card Precision top-$k$*. 
