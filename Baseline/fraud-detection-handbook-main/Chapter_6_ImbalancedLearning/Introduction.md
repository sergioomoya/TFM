(Imbalanced_Learning)=
# Introducción

El aprendizaje desequilibrado aborda problemas de clasificación donde el número de ejemplos que representan una clase es mucho menor que los de las otras clases. Aprender de conjuntos de datos desequilibrados es una tarea difícil, ya que la mayoría de los algoritmos de aprendizaje no están diseñados para hacer frente a una gran diferencia entre el número de casos que pertenecen a diferentes clases. Los algoritmos de clasificación a menudo están sesgados hacia los ejemplos de la clase mayoritaria, por lo que los minoritarios no están bien modelados en el sistema final {cite}`fernandez2018learning,chawla2009data,chawla2004special`. 

El fraude con tarjetas de crédito es un ejemplo de problema desequilibrado, ya que la proporción de fraudes en conjuntos de datos del mundo real puede ser tan baja como 0.01\% {cite}`lucas2020credit,ECB2020,dal2015adaptive`.  Los conjuntos de datos desequilibrados se encuentran más generalmente en muchos dominios de aplicación como las telecomunicaciones, la bioinformática o el diagnóstico médico, y se han considerado uno de los diez problemas principales en la minería de datos y el reconocimiento de patrones {cite}`JMLR:v18:16-365`. 

Se han propuesto muchas soluciones para tratar este problema, tanto para algoritmos de aprendizaje estándar como para técnicas de ensamblaje. Las técnicas de aprendizaje desequilibrado se pueden clasificar ampliamente en métodos *sensibles al costo* y *de remuestreo*. En los métodos *sensibles al costo*, los algoritmos se ajustan para favorecer la detección de la clase minoritaria. Esto generalmente implica una modificación de la función de optimización en el paso de entrenamiento del algoritmo de aprendizaje. Por el contrario, los métodos de *remuestreo* operan a nivel de datos, añadiendo un paso de preprocesamiento para reequilibrar el conjunto de datos antes de aplicar el algoritmo de entrenamiento. El remuestreo se puede lograr eliminando ejemplos de la clase mayoritaria (técnicas de *submuestreo*), añadiendo ejemplos de la clase minoritaria (técnicas de *sobremuestreo*), o confiando en una combinación de submuestreo y sobremuestreo  {cite}`fernandez2018learning,dal2015adaptive`.   

El capítulo está estructurado de la siguiente manera. La [Sección 6.2](Cost_Sensitive_Learning) cubre primero los métodos sensibles al costo. La [Sección 6.3](Resampling_Strategies) cubre luego las estrategias de remuestreo. La [Sección 6.4](Ensembling_Strategies) cubre finalmente los métodos que se basan en técnicas de ensamblaje. 
